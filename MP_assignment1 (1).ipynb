{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7047c9",
   "metadata": {},
   "source": [
    "# Homework Assignment: Building and Comparing High-Performance FashionMNIST Models\n",
    "\n",
    "**Objective:**\n",
    "Based on the attached notebook, further develop models for the FashionMNIST dataset that are highly accurate, lightweight, and optimized for fast inference. In this assignment you will experiment with:\n",
    "\n",
    "- One custom CNN model.\n",
    "- Five pretrained models (with some layers frozen).\n",
    "- Five different sampling strategies.\n",
    "- Various optimizers and learning schedulers.\n",
    "- Different random seeds.\n",
    "- Experimentation with loss functions on the best validation model.\n",
    "- Model inference optimization using `torch.compile` (PyTorch 2.0+).\n",
    "\n",
    "Your final deliverable should compare performance (accuracy, inference speed, model size) across these settings and include explanations for your design choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865c893",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation and Visualization\n",
    "\n",
    "**Question 1:**\n",
    "Examine the attached code for the FashionMNIST data loader.\n",
    "\n",
    "- **Task:** Explain how the `FashionMNISTSubset` class gathers images and labels, and why a base transform is applied before splitting the data, with heavier augmentation later applied to the training set.\n",
    "\n",
    "_(Write your explanation in a markdown cell below.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c738e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Data Loader Code\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class FashionMNISTSubset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths, self.labels = [], []\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(sorted(os.listdir(root_dir)))}\n",
    "        for class_name, idx in self.class_to_idx.items():\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(idx)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Define transforms\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8):\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    return random_split(dataset, [train_size, test_size])\n",
    "\n",
    "def get_dataloaders(root_dir, batch_size=32):\n",
    "    dataset = FashionMNISTSubset(root_dir, transform=base_transform)\n",
    "    train_set, test_set = split_dataset(dataset)\n",
    "    # Override training set transform with augmentation\n",
    "    train_set.dataset.transform = train_transform\n",
    "    # Using a WeightedRandomSampler for class imbalance\n",
    "    class_counts = torch.bincount(torch.tensor([dataset.labels[i] for i in train_set.indices]))\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    sample_weights = [class_weights[dataset.labels[i]] for i in train_set.indices]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load dataloaders\n",
    "root_dir = \"fashion_mnist_subset\"  # Adjust if needed\n",
    "train_loader, test_loader = get_dataloaders(root_dir)\n",
    "\n",
    "def visualize_samples(dataloader, class_names, num_samples=6):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 6))\n",
    "    for i in range(num_samples):\n",
    "        img = images[i].squeeze().numpy()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(class_names[labels[i].item()])\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "class_names = sorted(os.listdir(root_dir))\n",
    "\n",
    "# Visualize a training batch\n",
    "for images, labels in train_loader:\n",
    "    print(\"Train Batch:\", images.shape, labels.shape)\n",
    "    visualize_samples(train_loader, class_names)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156de13d",
   "metadata": {},
   "source": [
    "## Part 2: Model Development â€“ Custom and Pretrained Models\n",
    "\n",
    "**Question 2a: Custom Model**\n",
    "\n",
    "- **Task:** Design and implement a custom CNN model that is compact, fast, and accurate for FashionMNIST. Include your training loop and evaluation code.\n",
    "\n",
    "_(Discuss your design choices in a markdown cell.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.fc1   = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2   = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the custom model\n",
    "custom_model = CustomCNN(num_classes=10).to(device)\n",
    "print(custom_model)\n",
    "\n",
    "# TODO: Add your training and evaluation code for the custom model here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6c64e",
   "metadata": {},
   "source": [
    "**Question 2b: Pretrained Models**\n",
    "\n",
    "- **Task:** Adapt and fine-tune five pretrained models (e.g., ResNet18, MobileNet_V2, EfficientNet, VGG16, DenseNet121). Freeze some early layers and replace the final classifier to output 10 classes.\n",
    "\n",
    "_(Explain your modifications and training strategy in a markdown cell.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Example with ResNet18\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "for name, param in pretrained_model.named_parameters():\n",
    "    if \"layer1\" in name or \"layer2\" in name or \"layer3\" in name:\n",
    "        param.requires_grad = False\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "print(pretrained_model)\n",
    "\n",
    "# TODO: Add your training and evaluation code for the pretrained model here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135aa9ef",
   "metadata": {},
   "source": [
    "**Question 2c: Loss Functions Experiment**\n",
    "\n",
    "- **Task:** For the best validation model from Part 2b, experiment with at least two different loss functions (e.g., CrossEntropyLoss vs. a custom loss or Label Smoothing loss). Report how each loss function impacts convergence and final accuracy.\n",
    "\n",
    "_(Explain your findings in a markdown cell.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loss Functions Experiment\n",
    "criterion_standard = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom loss function example (modify as needed)\n",
    "def custom_loss(output, target):\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "    return loss * 1.0  # Adjust scaling if necessary\n",
    "\n",
    "# Assume 'best_pretrained_model' is the best model from Part 2b\n",
    "# Replace dummy_input and dummy_target with your actual batch data\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Adjust dimensions for pretrained model\n",
    "dummy_target = torch.tensor([0]).to(device)\n",
    "\n",
    "outputs = pretrained_model(dummy_input)\n",
    "loss_standard = criterion_standard(outputs, dummy_target)\n",
    "loss_custom = custom_loss(outputs, dummy_target)\n",
    "\n",
    "print(\"Standard Loss:\", loss_standard.item())\n",
    "print(\"Custom Loss:\", loss_custom.item())\n",
    "\n",
    "# TODO: Record your observations and compare convergence behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ddbf1",
   "metadata": {},
   "source": [
    "## Part 3: Experimenting with Sampling Strategies, Optimizers, and Learning Schedulers\n",
    "\n",
    "**Question 3a: Sampling Strategies**\n",
    "\n",
    "- **Task:** Implement and compare at least five different sampling strategies in your DataLoader.\n",
    "\n",
    "_(Describe in a markdown cell the scenarios in which each strategy might be preferred.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler, RandomSampler, SubsetRandomSampler, BatchSampler\n",
    "\n",
    "# Example using SequentialSampler\n",
    "sequential_sampler = SequentialSampler(train_loader.dataset)\n",
    "sequential_loader = DataLoader(train_loader.dataset, batch_size=16, sampler=sequential_sampler)\n",
    "\n",
    "# TODO: Experiment with other samplers and compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c371a",
   "metadata": {},
   "source": [
    "**Question 3b: Optimizers and Learning Schedulers**\n",
    "\n",
    "- **Task:** Experiment with at least two optimizers (e.g., Adam, SGD with momentum) and two learning schedulers (e.g., StepLR, CosineAnnealingLR). Compare their effects on convergence and final accuracy.\n",
    "\n",
    "_(Include training curves and discussion in a markdown cell.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80161268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for the custom model:\n",
    "optimizer_custom = optim.Adam(custom_model.parameters(), lr=0.001)\n",
    "scheduler_custom = optim.lr_scheduler.StepLR(optimizer_custom, step_size=5, gamma=0.1)\n",
    "\n",
    "# TODO: Insert your training loop and plot loss/accuracy curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af62b65",
   "metadata": {},
   "source": [
    "**Question 3c: Random Seed Sensitivity**\n",
    "\n",
    "- **Task:** Train at least one model using three different random seeds and report the impact on training stability and accuracy.\n",
    "\n",
    "_(Explain your observations in a markdown cell.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [42, 123, 2021]:\n",
    "    torch.manual_seed(seed)\n",
    "    print(f\"Training with random seed: {seed}\")\n",
    "    # TODO: Reinitialize your model, optimizer, DataLoader as needed, and run your training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658a903",
   "metadata": {},
   "source": [
    "## Part 4: Inference Optimization with Model Compilation\n",
    "\n",
    "**Question 4:**\n",
    "\n",
    "- **Task:** Use PyTorch 2.0â€™s `torch.compile` to optimize your model's inference speed. Compare inference times before and after compilation using a dummy input.\n",
    "\n",
    "_(Discuss the speedup and any limitations in a markdown cell.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29714af",
   "metadata": {},
   "source": [
    "## Part 5: Final Analysis and Reflection\n",
    "\n",
    "**Question 5:**\n",
    "\n",
    "- **Task:** Compare the performance of all models and configurations (custom vs. pretrained, sampling strategies, optimizer/scheduler combinations, random seeds, loss functions, and inference optimization). Create a summary table of key metrics (accuracy, model size, inference time).\n",
    "\n",
    "_(Write a detailed discussion of the trade-offs observed in a markdown cell.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample results table (update with your experimental data)\n",
    "results = {\n",
    "    \"Model\": [\"Custom\", \"ResNet18\", \"MobileNet_V2\", \"EfficientNet\", \"VGG16\", \"DenseNet121\"],\n",
    "    \"Accuracy (%)\": [92.5, 93.0, 92.8, 93.5, 91.7, 92.0],\n",
    "    \"Params (M)\": [0.5, 11.7, 3.5, 5.3, 138, 8.0],\n",
    "    \"Inference Time (ms)\": [5.2, 12.0, 7.5, 9.0, 15.0, 8.5]\n",
    "}\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de37434",
   "metadata": {},
   "source": [
    "## Final Reflection\n",
    "\n",
    "**Question 6:**\n",
    "\n",
    "- **Task:** Write a concluding summary discussing the trade-offs between model accuracy, size, and inference speed; how different sampling strategies, optimizers/schedulers, loss functions, and random seeds affected training; and which configurations would be most suitable for production.\n",
    "\n",
    "_(Provide your final reflection in a markdown cell.)_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
